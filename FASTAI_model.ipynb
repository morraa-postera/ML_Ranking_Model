{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains a FastAI tabular model on preprocessed 'fingerprint difference' data and outputs a trained model ready to be used in inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.tabular import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'noncovalent'\n",
    "\n",
    "if dataset is 'acry':\n",
    "    with open('acry_model_data', 'rb') as filehandle:\n",
    "        data = pickle.load(filehandle)\n",
    "\n",
    "elif dataset is 'noncovalent':\n",
    "    with open('noncovalent_model_data', 'rb') as filehandle:\n",
    "        data = pickle.load(filehandle)\n",
    "\n",
    "elif dataset is 'combined':\n",
    "    with open('combined_model_data', 'rb') as filename:\n",
    "        data = pickle.load(filename)\n",
    "\n",
    "elif dataset is 'PLPro':\n",
    "    with open('PLPro_model_data', 'rb') as filehandle:\n",
    "        data = pickle.load(filehandle)\n",
    "\n",
    "else:\n",
    "    print('Sadly no such dataset exists')\n",
    "    \n",
    "X_train, y_train, X_valid, y_valid = np.array(data[0]), np.array(data[1]), np.array(data[2]), np.array(data[3])\n",
    "\n",
    "# Preprocess data using PCA\n",
    "preprocess = PCA(n_components = 20)\n",
    "X_train = preprocess.fit_transform(X_train)\n",
    "X_valid = preprocess.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check class balance -- both valid and train should be perfectly 50:50 by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Valid' # train or valid\n",
    "\n",
    "if data is 'train':\n",
    "    data = y_train\n",
    "else:\n",
    "    data = y_valid\n",
    "all_classes = np.unique(data)\n",
    "for class_id in all_classes:\n",
    "    print('Class ' + str(class_id) + ': ' + str(100 * np.count_nonzero(data == class_id)/len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We convert the training and validation arrays into dataframes for input into FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(value):\n",
    "    if value == 1: return 'Higher_Activity'\n",
    "    elif value == -1: return 'Lower_Activity'\n",
    "    else: print('Unknown value: ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Feature ' + str(i) for i in range(X_train.shape[1])]\n",
    "\n",
    "df_train = pd.DataFrame(X_train, columns = columns)\n",
    "df_train['Target'] = [relabel(value) for value in y_train]\n",
    "\n",
    "df_valid = pd.DataFrame(X_valid, columns = columns)\n",
    "df_valid['Target'] = [relabel(value) for value in y_valid]\n",
    "\n",
    "df = df_train.append(df_valid).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we use the FASTAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "classes = ['Lower_Activity', 'Higher_Activity']\n",
    "valid_idx = range(len(df)-len(df_valid), len(df))\n",
    "data = TabularDataBunch.from_df(path, df, 'Target', valid_idx=valid_idx, classes = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We found that the default parameters resulted in easy overfitting so we use a smaller network with non-negligible dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = tabular_learner(data, layers=[10, 5], ps = [0.3], metrics=[accuracy, AUROC()])\n",
    "learner.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(5, slice(1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of model performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def roc_plot(y_truth, y_probs):\n",
    "    fpr, tpr, _ = roc_curve(y_truth.numpy(), y_probs.numpy())\n",
    "    auc_score = roc_auc_score(y_truth.numpy(), y_probs.numpy())\n",
    "    roc_df = pd.DataFrame(columns = ['False Positive Rate', 'True Positive Rate'] )\n",
    "    roc_df['False Positive Rate'], roc_df['True Positive Rate'] = fpr, tpr\n",
    "        \n",
    "    plt.figure(figsize = [8, 6] )\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label = 'ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Validation ROC (Train/Test diffs)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-saved model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = load_learner('', 'noncovalent_model.pkl', test = TabularList.from_df(df_valid))\n",
    "y = torch.LongTensor([classes.index(row['Target']) for index, row in df_valid.iterrows()])\n",
    "preds,_,losses = learner.get_preds(ds_type=DatasetType.Test, with_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...or use the live trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y,losses = learner.get_preds(with_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation(learner, preds, y, losses)\n",
    "interp.plot_confusion_matrix(cmap = 'Greens', normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs, train_targets = learner.get_preds(DatasetType.Train)\n",
    "train_hits = np.argmax(train_probs, 1).numpy() == train_targets.numpy()\n",
    "\n",
    "valid_probs, valid_targets = learner.get_preds(DatasetType.Valid)\n",
    "valid_hits = np.argmax(valid_probs, 1).numpy() == valid_targets.numpy()\n",
    "\n",
    "print('Train Accuracy: ', str(np.round(100 * train_hits.sum()/len(train_hits),2)), '%')\n",
    "print('Valid Accuracy: ', str(np.round(100 * valid_hits.sum()/len(valid_hits),2)), '%')\n",
    "print('Train AUC: ', str(np.round(100 * roc_auc_score(train_targets.numpy(), np.array(list(zip(*train_probs))[1])),2)), '%')\n",
    "print('Valid AUC: ', str(np.round(100 * roc_auc_score(valid_targets.numpy(), np.array(list(zip(*valid_probs))[1])),2)), '%')\n",
    "print('Train LogLoss: ', str(np.round(log_loss(train_targets.numpy(), train_probs.numpy()),2)))\n",
    "print('Valid LogLoss: ', str(np.round(log_loss(valid_targets.numpy(), valid_probs.numpy()),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.export(file = 'noncovalent_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem-dl-fastai-env",
   "language": "python",
   "name": "chem-dl-fastai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
