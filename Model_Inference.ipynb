{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook takes in a trained model (and its relevant data) and performs inference on a given screening library. At the end we output compounds from the screening library that are predicted to have a higher potency than the top few known actives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import pdb\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "from fastai.basics import *\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Morgan_Fingerprint(smile):\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smile),3,nBits=512, useFeatures = True)\n",
    "\n",
    "def Atom_Pair(smile):\n",
    "    return rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(Chem.MolFromSmiles(smile),nBits = 512)\n",
    "\n",
    "def TopologicalTorsion(smile):\n",
    "    return rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(Chem.MolFromSmiles(smile),nBits = 512)\n",
    "\n",
    "def fingerprints(smile):\n",
    "    MF = Morgan_Fingerprint(smile)\n",
    "    AP = Atom_Pair(smile)\n",
    "    TT = TopologicalTorsion(smile)\n",
    "    return np.array(MF + AP + TT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What model to use? What screening library for inference shall we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'Model_Build/' # contains trained models and the relevant underlying data, both processed and raw\n",
    "model_file = 'noncovalent_model.pkl' # trained models\n",
    "model_data = 'noncovalent_model_data' # processed data\n",
    "raw_data = 'known_noncovalent_activity.csv' # raw data\n",
    "\n",
    "##############\n",
    "\n",
    "screening_library_folder_dir = 'Screening_Library/noncovalent_library' # directory where the screening libraries are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we pull in the original training data in order to apply the same PCA transforms to the new inference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(main_dir + model_data, 'rb') as filehandle:\n",
    "    X_train = pickle.load(filehandle)[0]\n",
    "\n",
    "preprocess = PCA(n_components = 20).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in the pre-saved FASTAI model and make predictions -- [ Lower_Activity, Higher_Activity ] are the classes here. So a prediction of '1' means first compound is predicted to have higher activity than the second compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to new inference data and then generated the required dataframe to pass to FASTAI model\n",
    "def fastai_preprocess(X, pca_preprocess):\n",
    "    X = pca_preprocess.transform(X)\n",
    "    columns = ['Feature ' + str(i) for i in range(X.shape[1])]\n",
    "    return pd.DataFrame(X, columns = columns)\n",
    "\n",
    "# Load pre-saved FASTAI model and run predictions...return tuple of probabilities as well as predicted class\n",
    "def fastai_inference(df_test):\n",
    "    learner = load_learner(main_dir, model_file, test = TabularList.from_df(df_test))\n",
    "    probs,_ = learner.get_preds(ds_type=DatasetType.Test)\n",
    "    return probs.numpy(), probs.argmax(dim = -1).numpy()\n",
    "\n",
    "# This provides a sense check that predicedt probabilities are correlated across the various known actives compounds.\n",
    "def prediction_correlation(probs_a, probs_b):\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Higher_Activity_than_compound_b', fontsize = 15)\n",
    "    ax.set_ylabel('Higher_Activity_than_compound_a', fontsize = 15)\n",
    "    ax.scatter(probs_b, probs_a, c = 'r', alpha=0.6)\n",
    "    fig.suptitle('Correlation: ' + str(np.round(100 * pearsonr(probs_a, probs_b)[0],1)) + '%', fontsize=14)\n",
    "    ax.plot()\n",
    "\n",
    "# Create an inference dataframe of length j x k where j is number of screening compounds and k is the number of known actives\n",
    "def inference_df(active_smiles, active_fps, candidate_smiles, candidate_fps):\n",
    "    all_df = []\n",
    "\n",
    "    for index, active in enumerate(active_fps):\n",
    "        X = candidate_fps - np.array(active) ## ALWAYS SCREENING COMPOUND - KNOWN ACTIVE COMPOUND\n",
    "        X_df = fastai_preprocess(X, preprocess) # now let's convert to a dataframe\n",
    "\n",
    "        probs, preds = fastai_inference(X_df)\n",
    "        more_active = preds == 1\n",
    "        print('Known Active ' + str(index + 1 ), ' : % predicted to be more potent than this active: ', str(100 * more_active.sum()/len(more_active)))\n",
    "\n",
    "        # Now build a nice dataframe\n",
    "        df = pd.DataFrame(data = candidate_smiles, columns = ['Candidate_SMILES'])\n",
    "        df['Known_Active_SMILES'] = active_smiles[index]\n",
    "        df['Higher_Activity'] = preds\n",
    "        df['Prob_Higher_Activity'] = list(zip(*probs))[1]\n",
    "        all_df.append(df)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the best set actives we have so far to act as the 'benchmark' for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_data = pd.read_csv(main_dir + raw_data).sort_values(by = 'IC50', ascending = True).reset_index(drop = True)\n",
    "active_smiles = known_data['SMILES'].to_list()[0:4]\n",
    "active_fps = [np.array(fingerprints(smi)) for smi in active_smiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now here are 2 ways to do inference. All produce 'final_df'; the dataframe of all predictions. The input screening library must contain 'SMILES' as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Multiple libraries in a folder -- assumes Fingerprints are pre-computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dfs = []\n",
    "path = screening_library_folder_dir\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    if 'Section' in file: # PostEra's preprocessing of large screening libraries broken up into many compressed CSVs labelled Section_X.pkl\n",
    "        with open(path + file, 'rb') as filehandle:\n",
    "            section_data = pd.read_pickle(path + file)\n",
    "        candidate_smiles, candidate_fps = section_data['SMILES'].to_list(), np.vstack(section_data['fingerprints'])\n",
    "        final_dfs.append(inference_df(active_smiles, active_fps, candidate_smiles, candidate_fps))\n",
    "final_df = [ pd.concat(df, ignore_index = True) for df in final_dfs]\n",
    "final_df = pd.concat(final_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: A single library. Will compute Fingerprints on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_library_file = 'model_time_split_data.csv'\n",
    "# section_data = pd.read_csv(single_library_file).drop_duplicates(subset = ['SMILES']).reset_index(drop = True).fillna(np.inf)\n",
    "# section_data['fingerprints'] = [fingerprints(row['SMILES']) for index, row in section_data.iterrows()]\n",
    "# candidate_smiles, candidate_fps = section_data['SMILES'].to_list(), np.vstack(section_data['fingerprints'])\n",
    "\n",
    "# final_dfs = inference_df(active_smiles, active_fps, candidate_smiles, candidate_fps)\n",
    "# final_df = pd.concat(final_dfs, ignore_index = True).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 2 columns if doing time-split dataset to see which compounds the model correctly identified as having higher IC50 than known top actives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Candidate_IC50'] = np.tile(section_data['IC50'].to_numpy(), len(active_smiles))\n",
    "# final_df['Known_Active_IC50'] = np.repeat(known_data.head(5)['IC50'].to_numpy(), section_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sense-Check here: Analyse correlation amongst predictions. Basically for 2 known actives we look at the correlation of their probabilities of being lower/higher potency than the screening compounds. Intuitively the correlation should be very high meaning that the candidate compounds are ranked consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_a, compound_b = 2,3\n",
    "rand_section = random.randint(0, len(final_dfs)-1)\n",
    "probs_a, probs_b = final_dfs[rand_section][compound_a]['Prob_Higher_Activity'], final_dfs[rand_section][compound_b]['Prob_Higher_Activity']\n",
    "\n",
    "prediction_correlation(probs_a, probs_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take all the results (final_df) and filter to find the compounds predicted to have a higher activity than all our baseline actives. Then also pivot the data to show a nice table of these new exciting compounds compared with known actives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_probs = final_df.groupby(by = 'Candidate_SMILES')['Prob_Higher_Activity'].min()\n",
    "min_hits = min_probs[min_probs > 0.5].to_frame().reset_index(level = 'Candidate_SMILES').sort_values(by = 'Prob_Higher_Activity', ascending = False)\n",
    "print('Number of hits that are predicted to have higher activity than all top actives: ', min_hits.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = final_df.drop_duplicates(subset = ['Candidate_SMILES', 'Known_Active_SMILES']).pivot(index = 'Candidate_SMILES', columns = 'Known_Active_SMILES', values = 'Prob_Higher_Activity')\n",
    "pivoted['Min_Probability'] = [ row.min() for index, row in pivoted.iterrows() ]\n",
    "pivoted = pivoted[pivoted['Min_Probability'] > 0.5]\n",
    "new_hits = pivoted.sort_values(by = ['Min_Probability'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the promising compounds somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'noncovalent_top_ranked_compounds.csv'\n",
    "new_hits.to_csv(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem-dl-fastai-env",
   "language": "python",
   "name": "chem-dl-fastai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
